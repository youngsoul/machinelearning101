{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Feature Selection\n",
    "\n",
    "In many cases we spend a good deal of time on *feature engineering* - the process of adding features to our model data.  But how do we know that the feature is helpful?  How can we be sure it is adding to the signal we are trying to measure and not just added noise.\n",
    "\n",
    "Adding more features to a model will make the model more complex and this will increase the likelyhood that the model will overfit the data.\n",
    "\n",
    "When you are confronted with a model with a large number of features, it is a good idea to ascertain if the feature is important, and if not, drop the feature.\n",
    "\n",
    "There are 3 ways to determine if a feature is important:\n",
    "\n",
    "#### Univariate Statistics\n",
    "\n",
    "#### Model-Based Solutions\n",
    "\n",
    "#### Iterative Selection\n",
    "\n",
    "All of these methods are supervised methods - meaning they need the target for fitting the model.  The data set to test whether the feature is important will be split into a training and test set, and a determination as to whether the feature is important will be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Statistics - (ANOVA) analysis of variance\n",
    "\n",
    "Univariate statistics approach looks at each feature individually, not their interactions or cummulative impact, and will compute whether there is a staticially significant relationship between the feature in question and the target value.\n",
    "\n",
    "The features with the highest confidence are selected to keep in the model, and the others are dropped.\n",
    "\n",
    "This method is usually fast to execute, because we are not building a model - but just looking for a relationship.  It is even independent of the model that will ultimately be executed.\n",
    "\n",
    "In **scikit-learn** you choose either the *f_classif* test for classification, or *f_regression* for regression.  A *p-value* is calculated, and if the *p-value* exceeds a threshold as determined by either using *SelectKBest* (select the best 'k' number of features) or *SelectPercentile* (select a fixed percentage of best features)\n",
    "\n",
    "Below is an example taken from the book:\n",
    "### Introduction to Machine Learning with Python: A Guide for Data Scientists\n",
    "- Book by Andreas Muller and Sarah Guido is a very consumeable guide to machine learning with great insight into the field.\n",
    "\n",
    "- You can find that on [Amazon](https://www.amazon.com/Introduction-Machine-Learning-Python-Scientists/dp/1449369413/ref=sr_1_2?ie=UTF8&qid=1511371961&sr=8-2&keywords=machine+learning+oreilly).\n",
    "\n",
    "- Andreas [GitHub](https://github.com/amueller/introduction_to_ml_with_python) material for the book.\n",
    "\n",
    "Using the *scikit-learn* cancer dataset with some random noise added to the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest, SelectFpr\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import f_classif \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a deterministic set of random numbers - which seems like an oxymoron, \n",
    "# but to keep the results consistent - add a seed\n",
    "rng = np.random.RandomState(42)\n",
    "# create a random normal distribution centered at zero, with a standard deviation of 1\n",
    "# with an output shape of len(cancer.data) number of rows, and 50 columns\n",
    "noise = rng.normal(loc=0.0, scale=1.0,size=(len(cancer.data), 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the shape of the Cancer dataset, and the shape of the noise we are adding.\n",
    "\n",
    "Notice that the both have the same number of rows, and the cancer dataset has 30 features, and the noise has 50 columns of random values.\n",
    "\n",
    "After we add the noise to the dataset, we can then apply the techniques of automatic feature selection to see if we can eliminate the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569, 50)\n"
     ]
    }
   ],
   "source": [
    "print(cancer.data.shape)\n",
    "print(noise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 80)\n"
     ]
    }
   ],
   "source": [
    "# Add noise to the features\n",
    "# first 30 are features from the dataset, the next 50 are noise\n",
    "X_with_noise = np.hstack([cancer.data, noise])\n",
    "print(X_with_noise.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data, via *train_test_split* and then apply the Univariate Statistics from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_with_noise, cancer.target, random_state=0, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f_classif and SelectPercentile to select 50% of the features\n",
    "\n",
    "The f_classif if the default test, but we are going to be explict.\n",
    "\n",
    "Also notice, that we are **NOT** creating a model, but using classes from the feature_selection package to fit and transform the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_percentile = SelectPercentile(score_func=f_classif, percentile=50)\n",
    "select_percentile.fit(X_train, y_train)\n",
    "\n",
    "# transform the training set\n",
    "X_train_select_percentile = select_percentile.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (284, 80)\n",
      "X_train_select_percentile.shape: (284, 40)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"X_train_select_percentile.shape: {X_train_select_percentile.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the number of features was reduced by 50%\n",
    "\n",
    "*SelectPercentile* has a method **get_support** which will return a boolean mask of all of the features selected. We can use this to get a visual representation of the features that have been selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], <a list of 0 Text yticklabel objects>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5IAAAA4CAYAAACPHscHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4k9FNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109a08438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = select_percentile.get_support()\n",
    "\n",
    "#visualize the mask -- black is True, white is False\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r') # reshape, 1 row all of the columns\n",
    "plt.xlabel('Selected Features')\n",
    "plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above you can see that some of the features selected, were actually in the noise - and randomly showed a more staticially significant relationship than the original features did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f_classif and SelectKBest 40 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (284, 80)\n",
      "X_train_select_kbest.shape: (284, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], <a list of 0 Text yticklabel objects>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5IAAAA4CAYAAACPHscHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4k9FNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108e26c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_kbest = SelectKBest(score_func=f_classif, k=40)\n",
    "select_kbest.fit(X_train, y_train)\n",
    "\n",
    "# transform the training set\n",
    "X_train_select_kbest = select_kbest.transform(X_train)\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"X_train_select_kbest.shape: {X_train_select_kbest.shape}\")\n",
    "mask = select_kbest.get_support()\n",
    "\n",
    "#visualize the mask -- black is True, white is False\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r') # reshape, 1 row all of the columns\n",
    "plt.xlabel('Selected Features')\n",
    "plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you would expect, it is exactly the same since we used the same scoring function.\n",
    "\n",
    "### f_classif and SelectFpr\n",
    "\n",
    "This selector will attempt to False Positive Rate\n",
    "\n",
    "Filter: Select the pvalues below alpha based on a FPR test.\n",
    "\n",
    "FPR test stands for False Positive Rate test. It controls the total amount of false detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (284, 80)\n",
      "X_train_select_fpr.shape: (284, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], <a list of 0 Text yticklabel objects>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5IAAAA4CAYAAACPHscHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAACmpJREFUeJzt3HusZWV9xvHv4wwjCAZERjMFFCw4\nZmplnBmso0IBLUGdKE3pBbQFY6Qxxqj1EtoaryGxMfGurQp0rEG80GJp/yiiQjFGKRwcZACxqCDI\nZWbSqmAtCP76x3rpHI/nzDnrXPc68/0kk7PXu9d617v3s/c65zfrXStVhSRJkiRJM/WopR6AJEmS\nJGlYLCQlSZIkSb1YSEqSJEmSerGQlCRJkiT1YiEpSZIkSerFQlKSJEmS1MucCskkpyS5JcmtSc6Z\nr0FpYSS5IMmOJNvHtR2c5PIk/9l+Pm4px6jJJTk8yRVJbkpyY5LXtXbzG4Ak+yb5jyTXt/ze2dqP\nTHJ1O4Z+LsmqpR6rJpdkRZJvJfnXtmx2A5HktiQ3JNmW5NrW5rFzAJIclOTiJN9JcnOSzWY3DEnW\ntu/cI/9+muT15re8zLqQTLIC+CjwQmAdcHqSdfM1MC2IrcApE9rOAb5SVUcDX2nLGj0PAW+sqnXA\ns4HXtO+b+Q3DA8BJVXUMsB44Jcmzgb8B3l9VRwH/DbxyCceoPXsdcPO4ZbMblhOran1VbWrLHjuH\n4YPAv1XV04Bj6L6DZjcAVXVL+86tBzYC/wNcgvktK3M5I/ks4Naq+n5VPQh8Fnjp/AxLC6GqrgL+\na0LzS4FPtcefAk5d1EFpRqrq7qq6rj2+j+6X6aGY3yBU5/62uE/7V8BJwMWt3fxGVJLDgBcD57Xl\nYHZD57FzxCU5EDgeOB+gqh6sqh9jdkP0fOB7VXU75reszKWQPBS4Y9zyna1Nw/LEqrq7Pb4HeOJS\nDkbTS3IE8EzgasxvMNrUyG3ADuBy4HvAj6vqobaKx9DR9QHgLcAv2/LjMbshKeBLScaSnN3aPHaO\nviOBncDft2nl5yXZH7Mboj8BLmqPzW8Z8WY7+n9VVXS/cDWikhwA/CPw+qr66fjnzG+0VdXDbYrP\nYXQzOp62xEPSDCTZAuyoqrGlHotm7XlVtYHuUpzXJDl+/JMeO0fWSmAD8LdV9UzgZ0yYBml2o69d\nP/4S4AsTnzO/4ZtLIfkj4PBxy4e1Ng3LvUnWALSfO5Z4PJpCkn3oisgLq+qfWrP5DUybmnUFsBk4\nKMnK9pTH0NH0XOAlSW6ju4TjJLrrtsxuIKrqR+3nDrprtJ6Fx84huBO4s6qubssX0xWWZjcsLwSu\nq6p727L5LSNzKSSvAY5ud65bRXfa+tL5GZYW0aXAme3xmcA/L+FYNIV2Tdb5wM1V9b5xT5nfACRZ\nneSg9ng/4PfornO9AjitrWZ+I6iq/rKqDquqI+h+z321ql6G2Q1Ckv2TPPaRx8DJwHY8do68qroH\nuCPJ2tb0fOAmzG5oTmf3tFYwv2Ul3VnlWW6cvIju2pEVwAVVde58DUzzL8lFwAnAIcC9wNuBLwKf\nB54E3A78UVVNvCGPlliS5wFfA25g93Vaf0V3naT5jbgkz6C7qcAKuv/A+3xVvSvJU+jOch0MfAt4\neVU9sHQj1Z4kOQF4U1VtMbthaDld0hZXAp+pqnOTPB6PnSMvyXq6m1ytAr4PvIJ2DMXsRl77z5sf\nAk+pqp+0Nr97y8icCklJkiRJ0t7Hm+1IkiRJknqxkJQkSZIk9WIhKUmSJEnqxUJSkiRJktSLhaQk\nSZIkqZc5F5JJzp6PgWhpmN9wmd2wmd+wmd9wmd2wmd9wmd3yMx9nJP1QDJv5DZfZDZv5DZv5DZfZ\nDZv5DZfZLTNObZUkSZIk9ZKqmvnKycxX1oLbuHFjr/XHxsYWpO8+/fYxCmPoq28mMzUqr0+zM9nn\nYufOnaxevXoJRqP5sNj5LdTxe280VXa+x6PHTGZnVN+3+TpuDu3vziF+NsfGxnZV1bRhWUgOWJ/s\nAJIsSN99+u1jFMbQV99MZmpUXp9mZ6E+F9p7LNTxW7v5Ho8eM5md5f6+De3vzoG+x2NVtWm69Zza\nKkmSJEnqxUJSkiRJktSLhaQkSZIkqRcLSUmSJElSLxaSkiRJkqReLCQlSZIkSb1YSEqSJEmSerGQ\nlCRJkiT1YiEpSZIkSeolVTXzlZOdwO0Tmg8Bds3noLSozG+4zG7YzG/YzG+4zG7YzG+4zG44nlxV\nq6dbqVchOWkHybVVtWlOnWjJmN9wmd2wmd+wmd9wmd2wmd9wmd3y49RWSZIkSVIvFpKSJEmSpF7m\no5D8xDz0oaVjfsNldsO2V+aX5K+T3Jjk20m2JfmdadbfmuS0WezniCRnzGK7SffX2n/QxrwNuLVv\n362fs5L8xmy21bzZK797y4j5DZfZLTMr59pBVfmhGDDzGy6zG7a9Mb8km4EtwIaqeiDJIcCqBdrd\nEcAZwGfmsc83V9XFc+zjLGA7cNdMN0iysqoemuN+1eyN373lxPyGy+yWH6e2SpIWyxpgV1U9AFBV\nu6rqLoAkG5P8e5KxJJclWTNx46nWSXJUki8nuT7JdUl+E3gPcFw7g/iGJCuSvDfJNe1s6J+3bZPk\nI0luSfJl4Al9XlCSk5N8o+33C0kOaO1va/vanuQTbT+nAZuAC9u49ktyWyuoSbIpyZXt8TuSfDrJ\n14FP72H8a5Jc1frbnuS4/rFIktSfhaQkabF8CTg8yXeTfCzJ7wIk2Qf4MHBaVW0ELgDOHb/hNOtc\nCHy0qo4BngPcDZwDfK2q1lfV+4FXAj+pqmOBY4FXJTkS+H1gLbAO+LO2/VTe+8jU1iS/3QrAtwIv\nqKoNwLXAX7R1P1JVx1bV04H9gC3tbOa1wMvauH4+zfu1rvV9+h7GfwZwWVWtB44Btk3TpyRJ82LO\nU1slSZqJqro/yUbgOOBE4HNJzqErrp4OXJ4EYAVdMTje2snWSfJY4NCquqTt438B2jrjnQw8Y9z1\njwcCRwPHAxdV1cPAXUm+uoeX8CtTW5NsoSv2vt72twr4Rnv6xCRvAR4DHAzcCPzLHt+gX3fpuGJz\nqvFfA1zQCu0vVpWFpCRpUVhISpIWTSvYrgSuTHIDcCYwBtxYVZv3sGkmW6cVkjMR4LVVddmE7V80\n07FP0efl7Yzh+D73BT4GbKqqO5K8A9h3ij4eYvfsoInr/GzCvn5t/G1/xwMvBrYmeV9V/UPvVyJJ\nUk9ObZUkLYoka5McPa5pPXA7cAuwut2MhyT7JPmtCZtPuk5V3QfcmeTU1v7oJI8B7gPGF5mXAa9u\nZ+5I8tQk+wNXAX/crkFcQ3emdKa+CTw3yVGtz/2TPJXdBeGuds3k+LvAThzXbcDG9vgP9rCvScef\n5MnAvVX1SeA8YEOP8UuSNGuekZQkLZYDgA8nOYjuTNytwNlV9WCbsvmhJAfS/W76AN10UACmWedP\ngY8neRfwC+APgW8DDye5HtgKfJDuTq7XpZuHuhM4FbgEOAm4Cfghu6emTquqdiY5C7goyaNb81ur\n6rtJPkl3d9Z76KafPmIr8HdJfg5sBt4JnJ/k3XRnaqdy3hTjPwF4c5JfAPfTXecpSdKCS1Ut9Rgk\nSZIkSQPi1FZJkiRJUi8WkpIkSZKkXiwkJUmSJEm9WEhKkiRJknqxkJQkSZIk9WIhKUmSJEnqxUJS\nkiRJktSLhaQkSZIkqZf/AxkgvSsJERZmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108e15a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_fpr = SelectFpr(score_func=f_classif, alpha=0.05)\n",
    "select_fpr.fit(X_train, y_train)\n",
    "\n",
    "# transform the training set\n",
    "X_train_select_fpr = select_fpr.transform(X_train)\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"X_train_select_fpr.shape: {X_train_select_fpr.shape}\")\n",
    "mask = select_fpr.get_support()\n",
    "\n",
    "#visualize the mask -- black is True, white is False\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r') # reshape, 1 row all of the columns\n",
    "plt.xlabel('Selected Features')\n",
    "plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, you can see that this feature selection method selected a different set of features.\n",
    "\n",
    "Which set of features produces the better model.\n",
    "\n",
    "Using LogisticRegression on the different X_train_select values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with all Features: 0.9298245614035088\n",
      "Score with KBest Features: 0.9403508771929825\n",
      "Score with Fpr Features: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# transform test data, because we have to apply the same transformation to the training and the test data\n",
    "X_test_select_kbest = select_kbest.transform(X_test)\n",
    "X_test_select_fpr = select_fpr.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "score = lr.score(X_test, y_test)\n",
    "\n",
    "lr.fit(X_train_select_kbest, y_train)\n",
    "score_kbest = lr.score(X_test_select_kbest, y_test)\n",
    "\n",
    "lr.fit(X_train_select_fpr, y_train)\n",
    "score_fpr = lr.score(X_test_select_fpr, y_test)\n",
    "\n",
    "print(f\"Score with all Features: {score}\")\n",
    "print(f\"Score with KBest Features: {score_kbest}\")\n",
    "print(f\"Score with Fpr Features: {score_fpr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Statistics Summary\n",
    "\n",
    "Reviewing the model results above, you can see that the removing features that are not statistically significant to the target, will increase the accurracy score.  This technique is computationally efficient, but it only considers individual features and not the interaction between features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Bases Feature Selection - TODO  page 240"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
