{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit Learns 'make_classification' \n",
    "\n",
    "The make_classification function is used to create a dataset for classification.  This notebook shows you how to use make_classification function to create a random sample dataset to work with.\n",
    "\n",
    "The documentation for this dataset can be found on the [scikit-learn website](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)\n",
    "\n",
    "Some of the options seem straightforward but other not so.\n",
    "\n",
    "- *n_features* provides for how many feature columns will be in the dataset.\n",
    "- *n_informative* indicates how many of the features are actually informative.  if *n_informative* is less than *n_features* then the resulting dataset will features that do not add new information, and that can be identified through feature selection techniques.\n",
    "- *n_redundant* the number of redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start will a very well behaved dataset\n",
    "\n",
    "- 3 features\n",
    "- all features are informatives\n",
    "- there are not redundant features\n",
    "- 2 target classes\n",
    "- the distribution is 50/50 of the target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features, target = make_classification(n_samples = 3000, \n",
    "                                       n_features = 3,\n",
    "                                       n_informative = 3,\n",
    "                                       n_redundant = 0,\n",
    "                                       n_classes = 2,\n",
    "                                       weights = [0.5, 0.5],\n",
    "                                       random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix:\n",
      " [[-0.02837016 -1.17901771 -1.9924315 ]\n",
      " [ 1.48936958 -1.35588181 -1.54431898]\n",
      " [ 0.22795969  0.30478455  0.84319136]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Feature Matrix:\\n {features[:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Vector: \n",
      " [1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f'Target Vector: \\n {target[:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91694352 0.92026578 0.910299   0.91       0.88       0.90333333\n",
      " 0.91666667 0.88294314 0.88628763 0.89966555]\n",
      "Average score: 0.9026404626718074\n"
     ]
    }
   ],
   "source": [
    "X = features\n",
    "y = target\n",
    "# 10-fold cross-validation with K=5 for KNN (the n_neighbors parameter)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# remember, cross_val_score, will stratify the training/testing set because the model used is a classification model.\n",
    "scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "print(scores)\n",
    "print(f\"Average score: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85714286 0.90365449 0.88039867 0.85666667 0.85       0.90333333\n",
      " 0.86       0.85953177 0.8729097  0.88628763]\n",
      "Average score: 0.8729925110279003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "# remember, cross_val_score, will stratify the training/testing set because the model used is a classification model.\n",
    "scores = cross_val_score(tree, X, y, cv=10, scoring='accuracy')\n",
    "print(scores)\n",
    "print(f\"Average score: {scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "For the given dataset from make_classification, we have the following results:\n",
    "\n",
    "- KNN CrossValidation Accuracy:    0.9026404626718074\n",
    "- DecisionTreeClassifier Accuracy: 0.8729925110279003\n",
    "\n",
    "Lets take a sample from train_test_split and see what the confusion matrix looks like for that sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Confusion Matrix:\n",
      "[[329  42]\n",
      " [ 27 352]]\n",
      "0.908\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_predictions = knn.predict(X_test)\n",
    "\n",
    "knn_confusion = confusion_matrix(y_test, knn_predictions)\n",
    "knn_score = knn.score(X_test, y_test)\n",
    "\n",
    "print(f\"Logistic Regression Confusion Matrix:\\n{knn_confusion}\")\n",
    "print(f'{knn_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Confusion Matrix:\n",
      "[[310  61]\n",
      " [ 28 351]]\n",
      "0.8813333333333333\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "tree_predictions = tree.predict(X_test)\n",
    "tree_confusion = confusion_matrix(y_test, tree_predictions)\n",
    "tree_score = tree.score(X_test, y_test)\n",
    "\n",
    "print(f\"Tree Confusion Matrix:\\n{tree_confusion}\")\n",
    "print(f'{tree_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "For the train_test_split of the sample data, the KNN model still performs better in terms of accuracy, but now we can see the False Positives and False Negatives.  From here we have to decide if we need to account for these based on our business problem.  Is it ok to have more False Negatives or False Positives?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- add another example of where the number of informative features is not the same as the number of features and use the techniques from the earlier notebooks to reduce the features.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
